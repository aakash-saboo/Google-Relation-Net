{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISE THE MODEL AND THEN CONVERT IT INTO MULTI-GPU EXPLICITLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "n_classes=10\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 50\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 2\n",
    "\n",
    "#embedding dimension\n",
    "embed_dim=100\n",
    "\n",
    "cuda =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features,ngpu):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (device.type=='cuda'):\n",
    "            x= x.to(device)\n",
    "            output=torch.nn.data_parallel(self.conv_block,input=x,device_ids=range(ngpu))\n",
    "            return(x + output)\n",
    "        \n",
    "        else:\n",
    "            return(x+self.conv_block(x)_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_extraction_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Feature_extraction_Model, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(24)\n",
    "        self.conv3 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(24)\n",
    "        self.conv4 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(24)\n",
    "\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \"\"\"convolution\"\"\"\n",
    "        \n",
    "        if (device.type=='cuda'):\n",
    "            img= img.to(device)\n",
    "        \n",
    "        x = self.conv1(img)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_theta(nn.Module):\n",
    "    ''' Gives the sum of all the relations '''\n",
    "    \n",
    "    def __init__(self,input_channels=63):\n",
    "        super(G_theta,self).__init__()\n",
    "        \n",
    "        self.g_fc1 = nn.Linear(input_channels, 256) \n",
    "\n",
    "        self.g_fc2 = nn.Linear(256, 256)\n",
    "        self.g_fc3 = nn.Linear(256, 256)\n",
    "        self.g_fc4 = nn.Linear(256, 256)\n",
    "        \n",
    "    def forward(self,pairs):\n",
    "        d=int((pairs.size()[0]/batch_size)**0.25)\n",
    "        \n",
    "        if (device.type=='cuda'):\n",
    "            pairs= pairs.to(device)\n",
    "\n",
    "        x_ = pairs\n",
    "        x_ = self.g_fc1(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc2(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc3(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc4(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        print('final x_ size = {}'.format(x_.size()))\n",
    "        x_g = x_.view(batch_size,d*d*d*d,256) \n",
    "        x_g = x_g.sum(1).squeeze()\n",
    "        \n",
    "        return(x_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "#     print('module={}'.format(m))\n",
    "#     print(\"classname={}\".format(classname))\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#         print(\"weights initialised with normal distribution\")\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "#         print(\"weights initialised with normal distribution and bias set to zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging():\n",
    "    d= conv.size()[2]\n",
    "    tag= torch.zeros((2,d,d))\n",
    "    print('tagging in process')\n",
    "    for i in range(d):\n",
    "        for j in range(d):    # Can speed up this loop by parallization\n",
    "            tag[0,i,j] = float(int(i%d))/(d-1)*2-1\n",
    "            tag[1,i,j] = float(int(j%d))/(d-1)*2-1\n",
    "    \n",
    "    return tag.view((conv.size()[0],2,d,d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_relations(nn.Module):\n",
    "    def __init__(self,conv_map):\n",
    "        super(Calculate_relations, self).__init__()\n",
    "        \n",
    "        \n",
    "        g_theta=G_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(tags,conv_map,condition_vector=[]):\n",
    "    \n",
    "    '''\n",
    "    d: height of conv_map\n",
    "    tags:torch tensor of size-(batchsize,2,d,d)\n",
    "    condition_vector=torch tensor of size-(batchsize,its_dimension)\n",
    "    \n",
    "    returns: final tensor of size (batch*height*width*height*width,channels) which can be used \n",
    "    directly into G_theta\n",
    "    '''\n",
    "\n",
    "    x=torch.cat([conv_map,tags],dim=1)\n",
    "    mb=x.size()[0]\n",
    "    n_channels = x.size()[1]\n",
    "    d = x.size()[2]\n",
    "    x_flat = x.view(mb,n_channels,d*d).permute(0,2,1) # (64x25x24+2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(not condition_vector is None):\n",
    "        condition_vector = torch.unsqueeze(condition_vector, 1) #64x1xq\n",
    "        \n",
    "        condition_vector = condition_vector.repeat(1,d*d,1) #64x25xq\n",
    "       \n",
    "        condition_vector = torch.unsqueeze(condition_vector, 2) # 64x25x1xq\n",
    "       \n",
    "        \n",
    "        # cast all pairs against each other\n",
    "        x_i = torch.unsqueeze(x_flat,1) # (64x1x25x26)\n",
    "        \n",
    "        x_i = x_i.repeat(1,d*d,1,1) # (64x25x25x26)\n",
    "        \n",
    "        x_j = torch.unsqueeze(x_flat,2) # (64x25x1x26)\n",
    "        \n",
    "        x_j = torch.cat([x_j,condition_vector],3) # (64x25x1x26+11)\n",
    "        \n",
    "        x_j = x_j.repeat(1,1,d*d,1) # (64x25x25x26+11)\n",
    "        \n",
    "        \n",
    "        # concatenate all together\n",
    "        x_full = torch.cat([x_i,x_j],3) # (64x25x25x2*26+11)\n",
    "        \n",
    "        \n",
    "        # reshape for passing through network G_theta\n",
    "        x_ = x_full.view(mb*d*d*d*d,x_full.size()[3])\n",
    "        input_channels=x_full.size()[3]\n",
    "        return(x_,input_channels,mb,d)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # cast all pairs against each other\n",
    "        x_i = torch.unsqueeze(x_flat,1) # (64x1x25x26)\n",
    "        \n",
    "        x_i = x_i.repeat(1,d*d,1,1) # (64x25x25x26)\n",
    "        \n",
    "        x_j = torch.unsqueeze(x_flat,2) # (64x25x1x26)\n",
    "        \n",
    "        x_j = x_j.repeat(1,1,d*d,1) # (64x25x25x26)\n",
    "        \n",
    "        # concatenate all together\n",
    "        x_full = torch.cat([x_i,x_j],3) # (64x25x25x2*26)\n",
    "        \n",
    "        \n",
    "        # reshape for passing through network G_theta\n",
    "        x_ = x_full.view(mb*d*d*d*d,x_full.size()[3])\n",
    "        input_channels=x_full.size()[3]\n",
    "        \n",
    "        return(x_,input_channels,mb,d)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_map = torch.rand((64,24,5,5))\n",
    "tags=torch.rand((64,2,5,5))\n",
    "condition_vector= torch.rand((64,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "z,input_channels=get_pairs(conv_map,tags,condition_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 63])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations(x_):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final x_ size = torch.Size([40000, 256])\n"
     ]
    }
   ],
   "source": [
    "g_theta=G_theta(input_channels=63)\n",
    "out=g_theta(pairs=z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
